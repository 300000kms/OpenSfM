#!/usr/bin/env python

import argparse
import datetime
import logging
import math
import numpy as np
import networkx as nx
import os.path
import shutil
import yaml


from opensfm import bow
from opensfm import commands
from opensfm import dataset
from opensfm import feature_loading
from opensfm import io
from opensfm import log
from opensfm import matching
from opensfm import pairs_selection


logger = logging.getLogger(__name__)
log.setup()


def create_bench_dir(dataset):
    base_dir = os.path.dirname(os.path.normpath(dataset)) if \
        os.path.isdir(dataset) else \
        os.path.dirname(dataset)

    dataset_dir = os.path.basename(os.path.normpath(dataset))

    bench_dir = os.path.join(
        base_dir,
        dataset_dir + \
        "__bench_feat_match__" + \
        datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S"))

    io.mkdir_p(bench_dir)

    return bench_dir


def create_dataset(orig_dir, bench_dir, config):
    if config["image_retrieval"] == "WORDS":
        return create_words_dataset(orig_dir, bench_dir, config)
    elif config["image_retrieval"] == "VLAD":
        return create_vlad_dataset(orig_dir, bench_dir, config)
    else:
        raise ValueError("Invalid image retreival type: {}".format(
            matcher_type))


def create_words_dataset(orig_dir, bench_dir, config):
    work_dir = os.path.join(bench_dir, "words")
    shutil.copytree(orig_dir, work_dir)

    overwrite_config(config, work_dir)

    parser = argparse.ArgumentParser()
    parser.add_argument('dataset')
    args = parser.parse_args([work_dir])

    data = dataset.DataSet(args.dataset)

    for image in data.images():
        bows = bow.load_bows(data.config)
        n_closest = data.config['bow_words_to_match']
        p, f, c = data.load_features(image)
        closest_words = bows.map_to_words(
            f, n_closest, data.config['bow_matcher_type'])
        data.save_words(image, closest_words)

    return data


def create_vlad_dataset(orig_dir, bench_dir, config):
    work_dir = os.path.join(bench_dir, "vlad")
    shutil.copytree(orig_dir, work_dir)

    overwrite_config(config, work_dir)

    parser = argparse.ArgumentParser()
    parser.add_argument('dataset')
    args = parser.parse_args([work_dir])

    return dataset.DataSet(args.dataset)


def load_config(work_dir):
    config = {}
    filepath = os.path.join(work_dir, 'config.yaml')
    if os.path.isfile(filepath):
        with open(filepath) as fin:
            new_config = yaml.safe_load(fin)
        if new_config:
            for k, v in new_config.items():
                config[k] = v

    return config


def save_config(config, work_dir):
    with open(os.path.join(work_dir, 'config.yaml'), 'w') as fout:
        yaml.dump(config, fout, default_flow_style=False)


def overwrite_config(config, work_dir):
    orig_config = load_config(work_dir)

    for key, value in config.items():
        orig_config[key] = value

    save_config(orig_config, work_dir)


def merge_dicts(x, y):
    z = x.copy()
    z.update(y)
    return z


def bow_retrieval_penalty(data, bench_matches):
    images = data.images()
    histograms = pairs_selection.load_histograms(data, images)

    matches = {}
    for im in images:
        _, order, other = pairs_selection.bow_distances(im, images, histograms)
        matches[im] = np.array(other)[order].tolist()

    penalty = 0
    for im in bench_matches[d]:
        bm = bench_matches[d][im]
        cm = matches[im]

        for index, im2 in enumerate(bm):
            if not im2 in cm:
                penalty += len(bm) - index
            penalty += abs(index - cm.index(im2))

    penalty /= float(len(data.images()))

    return penalty


def vlad_retrieval_penalty(data, bench_matches):
    ims = data.images()
    histograms = pairs_selection.vlad_histograms(ims, data)

    matches = {}
    for im in ims:
        _, order, other = pairs_selection.vlad_distances(im, ims, histograms)
        matches[im] = np.array(other)[order].tolist()

    penalty = 0
    for im in bench_matches[d]:
        bm = bench_matches[d][im]
        cm = matches[im]

        for index, im2 in enumerate(bm):
            if not im2 in cm:
                penalty += len(bm) - index
            penalty += abs(index - cm.index(im2))

    penalty /= float(len(data.images()))

    return penalty


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Benchmark a reconstruction')
    parser.add_argument('--dataset', help='path to the dataset to be processed')

    args = parser.parse_args()

    datasets = [args.dataset] if args.dataset else \
        [
            "data/benchmark_matching/amsterdam",
            "data/benchmark_matching/copenhagen",
            "data/benchmark_matching/lemans",
            "data/benchmark_matching/malmo",
            "data/benchmark_matching/malmohus",
            "data/benchmark_matching/molleberga",
            "data/benchmark_matching/portland",
            "data/benchmark_matching/stapeln",
        ]

    default_config = {
        'feature_type': 'HAHOG',
        'feature_root': 1,
        'feature_min_frames': 4000,
        'feature_process_size': 2048,
        'feature_use_adaptive_suppression': False,
        'save_initial_matches': True,
    }

    configs = [
        merge_dicts(default_config, { 'image_retrieval': 'WORDS' }),
        merge_dicts(default_config, { 'image_retrieval': 'VLAD' }),
    ]

    b_matches = {}
    feature_loader = feature_loading.FeatureLoader()

    for d in datasets:
        b_matches[d] = {}

        args.dataset = d
        overwrite_config(merge_dicts(default_config, { 'matcher_type': 'MATRIX' }), d)

        commands.extract_metadata.Command().run(args)
        commands.detect_features.Command().run(args)

        bd = dataset.DataSet(args.dataset)

        ims = bd.images()
        for i, im1 in enumerate(ims):
            im1_matches = {}
            _, f1, _ = feature_loader.load_points_features_colors(bd, im1)
            m1 = feature_loader.load_masks(bd, im1)
            f1_filtered = f1 if m1 is None else f1[m1]

            for im2 in ims[(i + 1):]:
                _, f2, _ = feature_loader.load_points_features_colors(bd, im2)
                m2 = feature_loader.load_masks(bd, im2)
                f2_filtered = f2 if m2 is None else f2[m2]

                im1_matches[im2] = matching.match_matrix_symmetric(f1_filtered, f2_filtered, bd.config)

            bd.save_matches(im1, im1_matches, 'initial_matches.pkl.gz')

        matches_graph = nx.Graph()
        for im in ims:
            matches_graph.add_node(im)

        for im1 in ims:
            matches = bd.load_matches(im1, 'initial_matches.pkl.gz')
            for im2 in matches:
                matches_graph.add_edge(im1, im2, { 'n': len(matches[im2]) })

        for im1 in ims:
            im1_matches = []
            for im2 in ims:
                if im1 == im2:
                    continue
                im1_matches.append([im2, matches_graph.get_edge_data(im1, im2)['n']])
            im1_matches = sorted(im1_matches, key=lambda m: m[1], reverse=True)
            b_matches[d][im1] = [i[0] for i in im1_matches]

    bench_results =  []
    for c in configs:
        bench_result = {
            "config": c,
            "datasets": [],
            "penalties": [],
        }

        for d in datasets:
            bench_dir = create_bench_dir(d)
            td = create_dataset(d, bench_dir, c)

            if c['image_retrieval'] == "WORDS":
                penalty = bow_retrieval_penalty(td, b_matches)
            elif c['image_retrieval'] == 'VLAD':
                penalty = vlad_retrieval_penalty(td, b_matches)
            else:
                raise ValueError("Invalid image retreival type: {}".format(
                    matcher_type))

            bench_result["datasets"].append(d)
            bench_result["penalties"].append(str(penalty))

        bench_results.append(bench_result)

    for br in bench_results:
        logger.info("========================================")
        run_definition = "Report - "
        for key, value in br['config'].items():
            run_definition += key + ": " + str(value) + ", "

        logger.info(run_definition)
        logger.info("Dataset:")
        logger.info("\t".join(br["datasets"]))
        logger.info("Penalty:")
        logger.info("\t".join(br["penalties"]))
        logger.info("========================================")
